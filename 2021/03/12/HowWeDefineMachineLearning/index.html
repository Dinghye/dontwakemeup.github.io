<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"dinghye.gitee.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="什么样的模型是我们期望的？我们如何从无限的假设空间中找到最合适的假设空间？本篇笔记主要从偏差、方差分解（BVD）以及贝叶斯、香农两个角度来说明以上的问题。">
<meta property="og:type" content="article">
<meta property="og:title" content="【机器学习】我们如何定义机器学习？">
<meta property="og:url" content="http://dinghye.gitee.io/2021/03/12/HowWeDefineMachineLearning/index.html">
<meta property="og:site_name" content="DontWakeMeUP">
<meta property="og:description" content="什么样的模型是我们期望的？我们如何从无限的假设空间中找到最合适的假设空间？本篇笔记主要从偏差、方差分解（BVD）以及贝叶斯、香农两个角度来说明以上的问题。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/03/15/ZN7BfE5IJm6DK1l.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/BMQtOApTD8yG9Wo.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/WGE4V9cQsPlnA7R.png">
<meta property="og:image" content="https://i.loli.net/2021/03/12/IzgeV4BhSiEoFOH.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/UAcaFks62w3SKhn.png">
<meta property="og:image" content="https://i.loli.net/2021/03/12/5k6ChrcvwgXqpx2.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/vKYiPkaBsNyc7fb.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/KgW27V53ZEjTbXq.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/Whz4kFxnSXTyMAD.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/xqiRHShyM1pcOTr.png">
<meta property="og:image" content="https://miro.medium.com/max/703/1*RQ6ICt_FBSx6mkAsGVwx8g.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/dnVsAieXKUyP8vw.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/qJtFApYcPOQEuV9.png">
<meta property="og:image" content="https://i.loli.net/2021/03/13/af5ivhroGE4CWqK.png">
<meta property="article:published_time" content="2021-03-12T07:56:18.000Z">
<meta property="article:modified_time" content="2021-08-02T07:51:28.000Z">
<meta property="article:author" content="Dingqi Ye">
<meta property="article:tag" content="基础原理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/03/15/ZN7BfE5IJm6DK1l.png">


<link rel="canonical" href="http://dinghye.gitee.io/2021/03/12/HowWeDefineMachineLearning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://dinghye.gitee.io/2021/03/12/HowWeDefineMachineLearning/","path":"2021/03/12/HowWeDefineMachineLearning/","title":"【机器学习】我们如何定义机器学习？"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>【机器学习】我们如何定义机器学习？ | DontWakeMeUP</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">DontWakeMeUP</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Stay Curious</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A5%BD%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="nav-number">1.</span> <span class="nav-text">0. 什么样的模型是好模型？</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E4%BB%8E%E6%96%B9%E5%B7%AE%E3%80%81%E5%81%8F%E5%B7%AE%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3"><span class="nav-number">2.</span> <span class="nav-text">1. 从方差、偏差角度理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AFBias-amp-Variance"><span class="nav-number">2.1.</span> <span class="nav-text">1.1 什么是Bias&amp;Variance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-BVD-bias-Variance-decomposition-%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3"><span class="nav-number">2.2.</span> <span class="nav-text">1.2 BVD: bias-Variance decomposition 偏差-方差分解</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E4%BB%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3"><span class="nav-number">3.</span> <span class="nav-text">2. 从贝叶斯角度理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 贝叶斯定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E9%A6%99%E5%86%9C%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 香农信息熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-MDL%E6%9C%80%E5%B0%8F%E6%8F%8F%E8%BF%B0%E9%95%BF%E5%BA%A6"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 MDL最小描述长度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.4.</span> <span class="nav-text">2.4 贝叶斯视角下的机器学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%9C%80%E5%A5%BD%E7%9A%84%E5%81%87%E8%AE%BE"><span class="nav-number">4.</span> <span class="nav-text">3. 最好的假设</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Dingqi Ye"
      src="/images/laptop.jpg">
  <p class="site-author-name" itemprop="name">Dingqi Ye</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">37</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Dinghye" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Dinghye" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:dinghyye@163.com" title="E-Mail → mailto:dinghyye@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://dinghye.gitee.io/2021/03/12/HowWeDefineMachineLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/laptop.jpg">
      <meta itemprop="name" content="Dingqi Ye">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DontWakeMeUP">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="【机器学习】我们如何定义机器学习？ | DontWakeMeUP">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【机器学习】我们如何定义机器学习？
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-03-12 15:56:18" itemprop="dateCreated datePublished" datetime="2021-03-12T15:56:18+08:00">2021-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2021-08-02 15:51:28" itemprop="dateModified" datetime="2021-08-02T15:51:28+08:00">2021-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%8E%9F%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">原理</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>什么样的模型是我们期望的？我们如何从无限的假设空间中找到最合适的假设空间？本篇笔记主要从偏差、方差分解（BVD）以及贝叶斯、香农两个角度来说明以上的问题。</p>
<span id="more"></span>

<p>在前面的学习里我们已经知道，机器学习的整体框架：</p>
<img src="https://i.loli.net/2021/03/15/ZN7BfE5IJm6DK1l.png" alt="机器学习框架" style="zoom:50%;" />

<ul>
<li>我们的<strong>目的</strong>是，通过某些方法来找到逼近规则f的规则g。</li>
<li>我们的<strong>假设</strong>是，我们观测的的数据都来自于一个规则f，但它是未知的。</li>
<li>我们的<strong>方式</strong>是，通过理解观察，从假设集中给出合理的<u>假设</u>（可以理解为各种模型），再通过一些<u>算法</u>找到逼近f的g。</li>
</ul>
<p>于是在这个框架当中，我们关键问题就是“如何找到逼近f的g”，而解决这个问题就不得不对模型、假设等问题进行进一步的思考。</p>
<h1 id="0-什么样的模型是好模型？"><a href="#0-什么样的模型是好模型？" class="headerlink" title="0. 什么样的模型是好模型？"></a>0. 什么样的模型是好模型？</h1><p>首先我们就得对g和f的相似度，也就是逼近度进行一个度量。这个过程有的时候我们也叫做<strong>正则化</strong>（regularize）</p>
<img src="https://i.loli.net/2021/03/13/BMQtOApTD8yG9Wo.png" style="zoom:80%;" />

<p>其中等号右边的叫做<strong>损失项</strong>。$(x_i,y_i)$是训练样本，$l$是损失函数，$H$是假设空间。很多时候，我们看到的是这种表达：</p>
<img src="https://i.loli.net/2021/03/13/WGE4V9cQsPlnA7R.png" style="zoom:80%;" />

<p>其中多了一项$\lambda\Omega(f)$，这个项就叫做<strong>正则项</strong>。其中，$\Omega(f)$是函数的复杂度，$\lambda$是超参数。（关于正则项的内容我们不在这里讨论，我们只是对损失项的不同理解进行讨论）</p>
<h1 id="1-从方差、偏差角度理解"><a href="#1-从方差、偏差角度理解" class="headerlink" title="1. 从方差、偏差角度理解"></a>1. 从方差、偏差角度理解</h1><p>我们的最终目的还是做一个预测，我们希望模型$g$在未知数据中表现好，即泛化能力强，所以我们应该优化如下问题<br>$$<br>argmin_{g\in H}E_{(x,y)}l(y,g(x))<br>$$</p>
<h2 id="1-1-什么是Bias-amp-Variance"><a href="#1-1-什么是Bias-amp-Variance" class="headerlink" title="1.1 什么是Bias&amp;Variance"></a>1.1 什么是Bias&amp;Variance</h2><p>假设我们有一个想要近似的未知目标函数（f）。现在，假设我们从定义为“真函数+噪声”的未知分布中提取了不同的训练集。</p>
<ul>
<li><p><strong>偏差</strong>是我们模型的平均预测和我们试图预测的正确值之间的差异。高bias模型对训练数据关注少，模型过于简化。它往往会导致训练和测试数据的高误差。</p>
<p>为了使用更正式的属于来描述它，我们设（规则g）为$\hat y$<br>$$<br>Bias &#x3D; E[\hat y]-y<br>$$<br>下图显示了不同线性回归模型，每个模型都适合不同的训练集(除了x&#x3D;-10、x&#x3D;6)两个点外。所有的这些假设都无法很好的逼近真实函数f，在这里，我们可以说偏差很大，因为：</p>
<blockquote>
<p>真实值和预测值之间的差异平均（这里，平均值表示“对训练集的期望”而不是“对训练集中示例的期望”）</p>
</blockquote>
<img src="https://i.loli.net/2021/03/12/IzgeV4BhSiEoFOH.png" alt="High Bias" style="zoom: 20%;" />
</li>
<li><p><strong>方差</strong>是一个给定的数据点或值的模型预测的可变性，它告诉我们数据的分布。高方差模型关注的是训练数据，而不是以前没有见过的数据。因此，这些模型在训练数据上表现很好，但在测试数据上有很高的错误率。</p>
<p>类似的，我们将方差定义为平方估计量的期望值减去估计量平方期望值之间的差：</p>
<img src="https://i.loli.net/2021/03/13/UAcaFks62w3SKhn.png" style="zoom:80%;" />

<p>下图显示了不同的未修剪的决策树模型，每个模型都适合不同的训练集。值得注意的是，这些假设非常适合训练数据。但是，如果我们考虑对训练集的期望，则平均假设将完全符合真实函数（假设噪声无偏且期望值为0），我们可以看到，方差很大，因为平均而言，预测与预测期望值相差很大。</p>
<img src="https://i.loli.net/2021/03/12/5k6ChrcvwgXqpx2.png" alt="High Variance" style="zoom:20%;" />


</li>
<li><p>我们可以通过下图直观的理解模型中Variance和Bias的关系。实际上，bias可以理解为模型的错误率，bias越小，表示模型对数据的描述能力越好，而Variance则可以理解为模型解的离散程度。</p>
</li>
</ul>
<img src="https://i.loli.net/2021/03/13/vKYiPkaBsNyc7fb.png" alt="机器学习中的方差和偏差" style="zoom: 67%;" />

<h2 id="1-2-BVD-bias-Variance-decomposition-偏差-方差分解"><a href="#1-2-BVD-bias-Variance-decomposition-偏差-方差分解" class="headerlink" title="1.2 BVD: bias-Variance decomposition 偏差-方差分解"></a>1.2 BVD: bias-Variance decomposition 偏差-方差分解</h2><p>我们回顾一下，以上，我们定义了：</p>
<ul>
<li><p>真实目标函数：$y &#x3D; f(x)$</p>
</li>
<li><p>预测目标值：$\hat y &#x3D; \hat f(x)&#x3D;g(x)$</p>
</li>
<li><p>平方损失：$S &#x3D; (y-\hat y)^2$</p>
<img src="https://i.loli.net/2021/03/13/KgW27V53ZEjTbXq.png" alt="" style="zoom:80%;" /></li>
</ul>
<p>然后！我们对两边同时求期望</p>
<img src="https://i.loli.net/2021/03/13/Whz4kFxnSXTyMAD.png" style="zoom:80%;" />



<p>欸这个时候你可能会问，原来的$2ab$项($2(y-E[\hat y])(E[\hat y]-\hat y)$)到哪里去拉？我们发现，这一项在求期望之后就变成0了</p>
<img src="https://i.loli.net/2021/03/13/xqiRHShyM1pcOTr.png" style="zoom:80%;" />

<p>这是平方误差损失成偏差和方差的经典分解。$bias^2$和$Variance$通常无法进行定量描述，通过下面定性的曲线我们可以知道，我们需要找到的模型，并不是单纯的bias小或者是variance小，而是两者之间，都较小的情况，即Total error较小的情况。</p>
<p><img src="https://miro.medium.com/max/703/1*RQ6ICt_FBSx6mkAsGVwx8g.png" alt="Bias-Variance Trade-off"></p>
<h1 id="2-从贝叶斯角度理解"><a href="#2-从贝叶斯角度理解" class="headerlink" title="2. 从贝叶斯角度理解"></a>2. 从贝叶斯角度理解</h1><p>从极大似然角度，我们需要优化的是如下的极大似然问题：<br>$$<br>argmax_\theta P(Y,X|\theta)<br>$$</p>
<h2 id="2-1-贝叶斯定理"><a href="#2-1-贝叶斯定理" class="headerlink" title="2.1 贝叶斯定理"></a>2.1 贝叶斯定理</h2><p>贝叶斯定理描述的，是我们在观测到事件B时，有多大的可能相信证据A。它描述为：<br>$$<br>P(A|B)&#x3D;\frac{P(B|A)*P(A)}{P(B)}<br>$$<br>举个例子，当你观察到你的邻居平时不太爱说话，并且作息规律，那么你认为他更可能是一个图书管理员还是一个推销员呢？我们可能会回答，他是一名图书管理员。然而实际情况却不是这样，在日常生活中，图书管理员的人数比销售员的人数要小的多（$P(A&#x3D;图书管理员)&lt;P(A&#x3D;推销员)$。通过贝叶斯定理，我们可以计算出，相信证据A&#x3D;推销员的可能性要更大一些。</p>
<p>同样的，应用到机器学习当中，我们就是想通过贝叶斯定理的<strong>极大似然估计</strong>，来量化：</p>
<blockquote>
<p>当我们观测到数据D时，有多大可能去相信假设h？</p>
</blockquote>
<p>$$<br>P(h|D)&#x3D;\frac{P(D|h)*P(h)}{P(D)}<br>$$</p>
<p>因此，我们需要对该问题进行优化求解：</p>
<img src="https://i.loli.net/2021/03/13/dnVsAieXKUyP8vw.png" style="zoom:80%;" />

<p>我们经过数学的简单求导，可以将问题简化到的求解上：<br>$$<br>argmin(-log_2P(D|h)-log_2P(h))<br>$$<br>到这里，我们就不得不提到一个人——香农了~</p>
<h2 id="2-2-香农信息熵"><a href="#2-2-香农信息熵" class="headerlink" title="2.2 香农信息熵"></a>2.2 香农信息熵</h2><p>香农信息熵主要描述的，是：</p>
<blockquote>
<p>用二进制描述概率为P的事件X需要的长度，单位是bit</p>
</blockquote>
<p>香农的公式是是：<br>$$<br>H(X)&#x3D;-\sum_x P(x)log_2[P(x)]<br>$$<br>同时，它还有一个重要的推论，即，事件X的最小描述长度为：$-log_2{P(x)}$</p>
<p>我们如何理解这个公式呢？以世界杯为例，小明错过了一整个赛季，但是却想要知道冠军是谁，小华不想直接告诉他。小明每问一个问题，向小华支付一元钱，小华就告诉他是否正确。</p>
<p>这个时候，小明比较划算的做法，就是把参赛的32支球队都编上号，然后提问“冠军球队在1-16号中吗？”，如果猜对了，就接着问“在1-8中吗”。因此，最多需要5次，小明就能猜出是哪支球队。所以这个信息量为5。</p>
<p>然而实际中，小明也许不要那么次才猜出来。因为实际中，西班牙、巴西、德国等这样的球队会比其他的队伍夺冠的概率大。因此，我们分组的时候，我们可以把少数几个最可能的球队分到一组，把其他的队伍分到另一组，再进行猜测。这时，我们三到四次就可以猜出来结果。香农指出，他的信息量准确的应该是：<br>$$<br>-(p_1\times logp1+p2\times logp_2+…+p_{32}\times logp_{32})<br>$$<br>其中，$p_1….p_{32}$是这32个球队夺冠的概率。可以得到，当所有球队夺冠概率相同时，信息熵就为5。</p>
<p>其实更直观的理解，比如，有一天小明跑进来，说“太阳从东边升起来了”。这句话对于我们来说，信息量非常小，因为我们大家都知道这个公理。而如果小明跑进来说，“太阳从西边升起来了”，那这句话信息量老大了，我们会想是我们疯了还是小明疯了，信息本身的信息量是非常大的。</p>
<h2 id="2-3-MDL最小描述长度"><a href="#2-3-MDL最小描述长度" class="headerlink" title="2.3 MDL最小描述长度"></a>2.3 MDL最小描述长度</h2><p>话说回来。我们回到最初的MAP当中，我们会发现，我们要找的h，实际上就是满足（D|h）和（h）长度最小的h。<br>$$<br>argmin(-log_2P(D|h)-log_2P(h))&#x3D; argmin(length(D|h)+length(h))<br>$$</p>
<ol>
<li><p><strong>我们如何理解length(D|h)呢？</strong></p>
<p>length(D|h)，实际上表示的就是<strong>给定假设是数据的长度</strong>。再详细一点，就是<strong>在给定h时，训练数据D的描述长度</strong>。我们如何理解呢？</p>
<p>不妨以牛顿第二定律作为例子。牛顿在最初的《自然哲学的数学原理》提出该定律的时候，就没有给出严格的数学证明。它更多的就像是对自然物体运动观察而做出的假设，并且描述的非常好非常漂亮。</p>
<p>而，这就是为什么，我们不需要记下所有m、a对应的F，<strong>仅仅只需要相信这个漂亮的假设</strong>，我们的所有需要的数字就都可以通过这个假设得到。而这使得Length（D|h）非常小。而如果数据与假设有很大的偏差，我们则可能需要对这些偏差是什么，它可能的解释进行详细的描述。</p>
<blockquote>
<p>因此，Length(D|h)简介的表达了“<strong>数据与给定假设的匹配程度</strong>”这个概念</p>
</blockquote>
<p>实际上，它也就是<strong>错误分类</strong>（misclassification）或者<strong>错误率</strong>（error rate）的概念。</p>
</li>
<li><p><strong>我们如何理解length(h)呢？</strong></p>
<img src="https://i.loli.net/2021/03/13/qJtFApYcPOQEuV9.png" alt="" style="zoom: 40%;" />

<p>即使没有一个对假设的“长度”的精确定义，我相信你肯定会认为左边的树（A）看起来更小或更短。当然，你是对的。因此，更短的假设就是，<u>它要么自由参数更少，要么决策边界更不复杂，或者这些属性的某种组合可以表示它的简洁性</u>。</p>
<p>著名的<strong>奥卡姆剃刀</strong>也是这一原理，它的原文是“如无必要，勿增实体”。用统计学的话来说，我们必须努力用最简单的假设来解释所有的数据。</p>
</li>
</ol>
<h2 id="2-4-贝叶斯视角下的机器学习"><a href="#2-4-贝叶斯视角下的机器学习" class="headerlink" title="2.4 贝叶斯视角下的机器学习"></a>2.4 贝叶斯视角下的机器学习</h2><p>因此，贝叶斯推理也告诉我们，最好的假设就是最小化两个项之和：<strong>假设的长度和错误率</strong>。有趣是这与前面偏差-方差角度下的Bias（错误率）和Variance（假设的长度）不谋而合。</p>
<p>实际上，这里也存在一种权衡。如果你用奥卡姆剃刀挂掉你的假设，你可能会得到一个简单的模型，一个无法获得所有数据的模型。因此，你必须提供更多的数据以获得更好的一致性。</p>
<p>另一方面，如果你创建了一个复杂的（长的）假设，你可能可以很好的处理你的训练数据，但实际上可能不是正确的假设，因为它违背了MAP原则，即假设熵是小的。</p>
<p><img src="https://i.loli.net/2021/03/13/af5ivhroGE4CWqK.png" alt="complexity of the model"></p>
<h1 id="3-最好的假设"><a href="#3-最好的假设" class="headerlink" title="3. 最好的假设"></a>3. 最好的假设</h1><p>因此，无论是从bias&amp;variance角度或是贝叶斯角度，我们都得到了最重要的两条法则：</p>
<blockquote>
<p>它需要有更小的长度，也需要有较低的错误率。</p>
</blockquote>
<p>它使（通常）无限大的假设空间变小，并引导我们走向一组高度可能的假设，我们可以对其进行最优编码，并努力找到其中的一组MAP假设。</p>
<p>实际上，这句话几乎涵盖了所有（有监督）机器学习</p>
<ul>
<li>线性模型的模型复杂度——选择多项式的程度，如何减少平方和残差。</li>
<li>神经网络架构的选择——如何在不过度拟合训练数据的同时达到良好的验证精度？</li>
<li>支持向量机正则化和kernel选择——软边界与硬边界之间的平衡，即用决策边界非线性来平衡精度</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Dingqi Ye
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://dinghye.gitee.io/2021/03/12/HowWeDefineMachineLearning/" title="【机器学习】我们如何定义机器学习？">http://dinghye.gitee.io/2021/03/12/HowWeDefineMachineLearning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/" rel="tag"># 基础原理</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/03/11/WhatIsTheSelf/" rel="prev" title="【哲学】什么是自我？">
                  <i class="fa fa-chevron-left"></i> 【哲学】什么是自我？
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/03/17/zenAndtheArtOfMotorcycleMaintenance/" rel="next" title="【读书笔记】禅与摩托车维修艺术：良质与朴质">
                  【读书笔记】禅与摩托车维修艺术：良质与朴质 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dingqi Ye</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">187k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:50</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.3/jquery.min.js" integrity="sha256-pvPw+upLPUjgMXY0G+8O0xUf+/Im1MZjXxxgOcBQBXU=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  


  <script src="/js/third-party/fancybox.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
